{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2 concerns adding measurement error into the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 10000\n",
    "n = 100\n",
    "secret_beta = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "y = \\beta x + \\epsilon\n",
    "$$\n",
    "\n",
    "We only have data on $\\tilde{x} = x + u$, where $u \\sim N(0, \\sigma_u^2)$\n",
    "\n",
    "The OLS estimator for $\\beta$ is\n",
    "\n",
    "$$\n",
    "\\hat{\\beta} = \\frac{cov(\\tilde{x}, y)}{var(\\tilde{x})} = \\frac{cov(x + u, \\beta x + \\epsilon)}{var(\\tilde{x})}\n",
    "$$\n",
    "\n",
    "$$\n",
    "plim \\hat{\\beta} = \\frac{\\sigma^2_x}{\\sigma^2_x + \\sigma^2_u} \\beta\n",
    "$$\n",
    "\n",
    "Thus, the OLS estimator $\\hat{\\beta}$ is biased. The bias is $\\hat{\\beta} - \\beta = -\\frac{\\sigma^2_u}{\\sigma^2_x + \\sigma^2_u} \\beta$\n",
    "\n",
    "Also, $\\sigma^2_x + \\sigma^2_u = var(\\tilde{x})$\n",
    "\n",
    "If $\\sigma^2_u$ is known, we can obtain an unbiased estimator $\\tilde{\\beta} = \\hat{\\beta} \\cdot \\frac{var(\\tilde{x})}{var(\\tilde{x}) - \\sigma^2_u}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we try different $\\sigma^2_u$ and $\\sigma^2_\\epsilon$ and calculate the bias and MSE before and after adjusting for biasness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimating the residual variance:\n",
    "\n",
    "$\\hat{\\epsilon} = y - \\hat{\\beta}\\tilde{x} = y - \\hat{\\beta}(x+u)$\n",
    "\n",
    "$\\epsilon = y - \\beta x$\n",
    "\n",
    "$\\hat{\\epsilon} = y - \\hat{\\beta}\\tilde{x} = y - \\hat{\\beta}(x+u) + (\\epsilon - (y - \\beta x)) = \\epsilon + (\\beta - \\hat{\\beta}) - \\hat{\\beta} u$\n",
    "\n",
    "The residual contains two additional sources of variation compared to the true error.\n",
    "\n",
    "- $(\\beta - \\hat{\\beta})$: $\\hat{\\beta}$ is biased towards zero, under measurement error the term $\\hat{\\beta} - \\beta$ does not vanish asymptotically\n",
    "- Additional variance due to measurement error in the regressor\n",
    "\n",
    "$\\text{plim } \\hat{\\sigma^2_\\epsilon} = \\sigma^2_\\epsilon + (1-\\lambda)^2 \\beta^2 \\sigma_x^2 + \\lambda^2 \\beta^2 \\sigma^2_u$\n",
    "\n",
    "Thus, the estimated $\\sigma^2_\\epsilon$ is biased upwards.\n",
    "\n",
    "$\\lambda = \\frac{\\sigma^2_x}{\\sigma^2_x + \\sigma^2_u}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normally distributed epsilon\n",
      "var_u=0.00, var_eps=1.50:  Unadjusted: Bias = -0.000338, MSE = 0.000439; Sigma bias = -0.001362\n",
      "var_u=0.00, var_eps=1.50:  Adjusted:   Bias = -0.000338, MSE = 0.000439; Sigma bias = -0.001362\n",
      "var_u=0.00, var_eps=2.00:  Unadjusted: Bias = 0.000294, MSE = 0.000593; Sigma bias = 0.006379\n",
      "var_u=0.00, var_eps=2.00:  Adjusted:   Bias = 0.000294, MSE = 0.000593; Sigma bias = 0.006379\n",
      "var_u=0.00, var_eps=2.50:  Unadjusted: Bias = -0.000154, MSE = 0.000727; Sigma bias = 0.003370\n",
      "var_u=0.00, var_eps=2.50:  Adjusted:   Bias = -0.000154, MSE = 0.000727; Sigma bias = 0.003370\n",
      "var_u=1.50, var_eps=1.50:  Unadjusted: Bias = -0.082730, MSE = 0.008739; Sigma bias = 5.756851\n",
      "var_u=1.50, var_eps=1.50:  Adjusted:   Bias = 0.001188, MSE = 0.002357; Sigma bias = 6.014166\n",
      "var_u=1.50, var_eps=2.00:  Unadjusted: Bias = -0.082322, MSE = 0.008851; Sigma bias = 5.759395\n",
      "var_u=1.50, var_eps=2.00:  Adjusted:   Bias = 0.001637, MSE = 0.002566; Sigma bias = 6.016850\n",
      "var_u=1.50, var_eps=2.50:  Unadjusted: Bias = -0.081905, MSE = 0.008893; Sigma bias = 5.754518\n",
      "var_u=1.50, var_eps=2.50:  Adjusted:   Bias = 0.002082, MSE = 0.002683; Sigma bias = 6.012114\n",
      "var_u=2.00, var_eps=1.50:  Unadjusted: Bias = -0.108606, MSE = 0.014087; Sigma bias = 7.556680\n",
      "var_u=2.00, var_eps=1.50:  Adjusted:   Bias = 0.001821, MSE = 0.003090; Sigma bias = 8.008480\n",
      "var_u=2.00, var_eps=2.00:  Unadjusted: Bias = -0.108449, MSE = 0.014188; Sigma bias = 7.565917\n",
      "var_u=2.00, var_eps=2.00:  Adjusted:   Bias = 0.002041, MSE = 0.003238; Sigma bias = 8.017928\n",
      "var_u=2.00, var_eps=2.50:  Unadjusted: Bias = -0.108059, MSE = 0.014285; Sigma bias = 7.573848\n",
      "var_u=2.00, var_eps=2.50:  Adjusted:   Bias = 0.002563, MSE = 0.003459; Sigma bias = 8.026500\n",
      "var_u=2.50, var_eps=1.50:  Unadjusted: Bias = -0.133753, MSE = 0.020590; Sigma bias = 9.303598\n",
      "var_u=2.50, var_eps=1.50:  Adjusted:   Bias = 0.002596, MSE = 0.003941; Sigma bias = 10.001013\n",
      "var_u=2.50, var_eps=2.00:  Unadjusted: Bias = -0.133383, MSE = 0.020583; Sigma bias = 9.304788\n",
      "var_u=2.50, var_eps=2.00:  Adjusted:   Bias = 0.003056, MSE = 0.004040; Sigma bias = 10.002847\n",
      "var_u=2.50, var_eps=2.50:  Unadjusted: Bias = -0.133794, MSE = 0.020805; Sigma bias = 9.345313\n",
      "var_u=2.50, var_eps=2.50:  Adjusted:   Bias = 0.002541, MSE = 0.004166; Sigma bias = 10.043277\n"
     ]
    }
   ],
   "source": [
    "print(\"Normally distributed epsilon\")\n",
    "\n",
    "for var_u in [0, 1.5, 2, 2.5]:\n",
    "    for var_eps in [1.5, 2, 2.5]:\n",
    "        bias_beta = np.array([])\n",
    "        sqerr_beta = np.array([])\n",
    "        bias_beta_adj = np.array([])\n",
    "        sqerr_beta_adj = np.array([])\n",
    "        \n",
    "        bias_sigma = np.array([])\n",
    "        bias_sigma_adj = np.array([])\n",
    "        \n",
    "        for _ in range(iterations):\n",
    "            x = np.linspace(-10,10,n)\n",
    "            x_obs = x + np.random.normal(scale=np.sqrt(var_u), size=n)\n",
    "            y = x*secret_beta + np.random.normal(scale=np.sqrt(var_eps), size=n)\n",
    "\n",
    "            beta_est = np.cov(x_obs, y)[0][1]/np.var(x_obs, ddof=1)\n",
    "            beta_est_adj = np.cov(x_obs, y)[0][1]/np.var(x_obs, ddof=1) * np.var(x_obs, ddof=1)/(np.var(x_obs, ddof=1) - var_u)\n",
    "            \n",
    "            sigma_est = np.sum((y - beta_est * x_obs) ** 2) / (n-1)\n",
    "            sigma_est_adj = np.sum((y - beta_est_adj * x_obs) ** 2) / (n-1)\n",
    "            \n",
    "            bias_beta = np.append(bias_beta, (beta_est-secret_beta))\n",
    "            sqerr_beta = np.append(sqerr_beta, (beta_est-secret_beta)**2)\n",
    "            bias_beta_adj = np.append(bias_beta_adj, (beta_est_adj-secret_beta))\n",
    "            sqerr_beta_adj = np.append(sqerr_beta_adj, (beta_est_adj-secret_beta)**2)\n",
    "            \n",
    "            bias_sigma = np.append(bias_sigma, sigma_est-var_eps)\n",
    "            bias_sigma_adj = np.append(bias_sigma_adj, sigma_est_adj-var_eps)\n",
    "\n",
    "        print(\"var_u=%.2f, var_eps=%.2f:  Unadjusted: Bias = %f, MSE = %f; Sigma bias = %f\"%(var_u, var_eps, bias_beta.mean(), sqerr_beta.mean(), bias_sigma.mean()))\n",
    "    \n",
    "        print(\"var_u=%.2f, var_eps=%.2f:  Adjusted:   Bias = %f, MSE = %f; Sigma bias = %f\"%(var_u, var_eps, bias_beta_adj.mean(), sqerr_beta_adj.mean(), bias_sigma_adj.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$var = \\frac{df}{df-2}$\n",
    "\n",
    "$df = \\frac{2 \\cdot var}{var - 1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T distributed epsilon\n",
      "var_u=0.00, var_eps=1.50:  Unadjusted: Bias = 0.000120, MSE = 0.000443; Sigma bias = 0.002170\n",
      "var_u=0.00, var_eps=1.50:  Adjusted:   Bias = 0.000120, MSE = 0.000443; Sigma bias = 0.002170\n",
      "var_u=0.00, var_eps=2.00:  Unadjusted: Bias = -0.000229, MSE = 0.000589; Sigma bias = -0.004330\n",
      "var_u=0.00, var_eps=2.00:  Adjusted:   Bias = -0.000229, MSE = 0.000589; Sigma bias = -0.004330\n",
      "var_u=0.00, var_eps=2.50:  Unadjusted: Bias = 0.000337, MSE = 0.000742; Sigma bias = 0.005373\n",
      "var_u=0.00, var_eps=2.50:  Adjusted:   Bias = 0.000337, MSE = 0.000742; Sigma bias = 0.005373\n",
      "var_u=1.50, var_eps=1.50:  Unadjusted: Bias = -0.081403, MSE = 0.008722; Sigma bias = 5.738366\n",
      "var_u=1.50, var_eps=1.50:  Adjusted:   Bias = 0.002650, MSE = 0.002605; Sigma bias = 5.996283\n",
      "var_u=1.50, var_eps=2.00:  Unadjusted: Bias = -0.081448, MSE = 0.008844; Sigma bias = 5.740207\n",
      "var_u=1.50, var_eps=2.00:  Adjusted:   Bias = 0.002591, MSE = 0.002726; Sigma bias = 5.998189\n",
      "var_u=1.50, var_eps=2.50:  Unadjusted: Bias = -0.081405, MSE = 0.009060; Sigma bias = 5.725360\n",
      "var_u=1.50, var_eps=2.50:  Adjusted:   Bias = 0.002650, MSE = 0.002981; Sigma bias = 5.983278\n",
      "var_u=2.00, var_eps=1.50:  Unadjusted: Bias = -0.107774, MSE = 0.014941; Sigma bias = 7.514018\n",
      "var_u=2.00, var_eps=1.50:  Adjusted:   Bias = 0.002831, MSE = 0.004364; Sigma bias = 7.966796\n",
      "var_u=2.00, var_eps=2.00:  Unadjusted: Bias = -0.106677, MSE = 0.014866; Sigma bias = 7.539314\n",
      "var_u=2.00, var_eps=2.00:  Adjusted:   Bias = 0.004062, MSE = 0.004561; Sigma bias = 7.993119\n",
      "var_u=2.00, var_eps=2.50:  Unadjusted: Bias = -0.106748, MSE = 0.014908; Sigma bias = 7.440686\n",
      "var_u=2.00, var_eps=2.50:  Adjusted:   Bias = 0.004020, MSE = 0.004579; Sigma bias = 7.894231\n",
      "var_u=2.50, var_eps=1.50:  Unadjusted: Bias = -0.130179, MSE = 0.022563; Sigma bias = 9.087688\n",
      "var_u=2.50, var_eps=1.50:  Adjusted:   Bias = 0.006785, MSE = 0.007699; Sigma bias = 9.790176\n",
      "var_u=2.50, var_eps=2.00:  Unadjusted: Bias = -0.131000, MSE = 0.023332; Sigma bias = 9.117667\n",
      "var_u=2.50, var_eps=2.00:  Adjusted:   Bias = 0.005892, MSE = 0.008325; Sigma bias = 9.819933\n",
      "var_u=2.50, var_eps=2.50:  Unadjusted: Bias = -0.129085, MSE = 0.022690; Sigma bias = 9.133617\n",
      "var_u=2.50, var_eps=2.50:  Adjusted:   Bias = 0.008182, MSE = 0.008214; Sigma bias = 9.838711\n"
     ]
    }
   ],
   "source": [
    "print(\"T distributed epsilon\")\n",
    "for var_u in [0, 1.5, 2, 2.5]:\n",
    "    for var_eps in [1.5, 2, 2.5]:\n",
    "        bias_beta = np.array([])\n",
    "        sqerr_beta = np.array([])\n",
    "        bias_beta_adj = np.array([])\n",
    "        sqerr_beta_adj = np.array([])\n",
    "        \n",
    "        bias_sigma = np.array([])\n",
    "        bias_sigma_adj = np.array([])\n",
    "        \n",
    "        for _ in range(iterations):\n",
    "            x = np.linspace(-10,10,n)\n",
    "            if var_u > 0:\n",
    "                x_obs = x + np.random.standard_t(df=2*var_u/(var_u-1), size=n)\n",
    "            else:\n",
    "                x_obs = x\n",
    "            y = x*secret_beta + np.random.standard_t(df=2*var_eps/(var_eps-1), size=n)\n",
    "\n",
    "            beta_est = np.cov(x_obs, y)[0][1]/np.var(x_obs, ddof=1)\n",
    "            beta_est_adj = np.cov(x_obs, y)[0][1]/np.var(x_obs, ddof=1) * np.var(x_obs, ddof=1)/(np.var(x_obs, ddof=1) - var_u)\n",
    "            \n",
    "            sigma_est = np.sum((y - beta_est * x_obs) ** 2) / (n-1)\n",
    "            sigma_est_adj = np.sum((y - beta_est_adj * x_obs) ** 2) / (n-1)\n",
    "            \n",
    "            bias_beta = np.append(bias_beta, (beta_est-secret_beta))\n",
    "            sqerr_beta = np.append(sqerr_beta, (beta_est-secret_beta)**2)\n",
    "            bias_beta_adj = np.append(bias_beta_adj, (beta_est_adj-secret_beta))\n",
    "            sqerr_beta_adj = np.append(sqerr_beta_adj, (beta_est_adj-secret_beta)**2)\n",
    "            \n",
    "            bias_sigma = np.append(bias_sigma, sigma_est-var_eps)\n",
    "            bias_sigma_adj = np.append(bias_sigma_adj, sigma_est_adj-var_eps)\n",
    "\n",
    "        print(\"var_u=%.2f, var_eps=%.2f:  Unadjusted: Bias = %f, MSE = %f; Sigma bias = %f\"%(var_u, var_eps, bias_beta.mean(), sqerr_beta.mean(), bias_sigma.mean()))\n",
    "    \n",
    "        print(\"var_u=%.2f, var_eps=%.2f:  Adjusted:   Bias = %f, MSE = %f; Sigma bias = %f\"%(var_u, var_eps, bias_beta_adj.mean(), sqerr_beta_adj.mean(), bias_sigma_adj.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion 1: MSE for T-distributed measurement errors and eps error is greater than Normal-distributed errors for all variance combination experimented.\n",
    "\n",
    "Conclusion 2: Estimation bias for $\\sigma^2_\\epsilon$ for T-distributed errors is smaller than that for Normal-distributed errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe because T-distribution has greater Kurtosis than normal distribution?\n",
    "\n",
    "\"Kurtosis is a statistical measure that defines how heavily the tails of a distribution differ from the tails of a normal distribution. In other words, kurtosis identifies whether the tails of a given distribution contain extreme values.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
