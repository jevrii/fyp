\documentclass{article}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage[ruled,vlined]{algorithm2e}
\setlength{\parskip}{1em}
\renewcommand{\algorithmcfname}{Procedure}
\title{Investigation of Non-normality in a Simple Errors-in-variables Model}
\author{Lee Chun Yin 3035469140}
\date{April 2021}

\begin{document}

\maketitle

\begin{abstract}
    This is the abstract of the paper.
\end{abstract}

\section{Introduction}

Consider the problem of regression through the origin with only one explanatory variable:

\[
y = \beta x + \epsilon
\]

In real life applications, usually we will first obtain pairs of observations $(\tilde{x}_i, \tilde{y}_i)$, then apply a model by performing linear regression on the data.
However, we only have observations on the observed $\tilde{x}_i = x_i + u_i$ and $\tilde{y}_i = y_i + v_i$, which have some additive error compared to the true $x$ and $y$ that these model assumptions are based on.
We further assume that the measurement errors $u_i$ and $v_i$ have mean zero and constant variances, and that the measurement error is uncorrelated and independent to the true values $x$ and $y$.
This gives rise to the \textit{errors-in-variables model}.
This imposes a different problem from the classical linear regression model, because the classical model assumes that the observed $\tilde{x}$ is nonrandom, that we have access to the true value of explanatory variable $x$ without any error. 

Furthermore, in the classical linear regression model, we often assume that the observed dependent variable $y$ is subject to some $\epsilon$ following $N(0, \sigma^2)$.
However, the normality assumption often does not hold for real life datasets.
For example, when the errors in the dataset have heavy-tails and/or have skewed shapes, then the normal assumption may not be appropriate.
For instance, when dealing with datasets with heavy-tailed errors, one of the practices is to assume $t$-distributed errors instead of normal-distributed errors,
as the light-tailedness of the normal distribution essentially implies that we assume that large errors occur with very low probability, which may not be true in datasets of poorer quality.
To make the situation more complicated, in practice non-normality in errors may happen in both the residual $\epsilon$ and the observation matter $u$.
Thus, there is a need to investigate the impacts of non-normality on the \textit{errors-in-variables model}. 

In this project, we investigate how the non-normality of errors in both the explanatory variable $x$ and the residual of the dependent variable $y$ affects the estimation of the regression coefficient $\hat{beta}$ and the estimation of variance of error $\sigma^2_\epsilon$.
We first perform a literature review on existing results on the \textit{errors-in-variables model}.
Then we will describe in detail the methodology computer simulation technique to produce results.
Finally, we will present the results and findings from the computer simulations.
The code used in this project can be found in the appendix.

\section{Literature review}

We mainly refer to the lecture notes written by Pischke \cite{lecturenotes} for the errors-in-variables model. 

Suppose we wish to estimate the relationship $y = \beta x + \epsilon$, but we only have data on $\tilde{x} = x + u$. Also, let's further assume that $\sigma_v^2 = 0$, i.e. there is only measurement error in $x$.

If we substitute $\tilde{x} = x+u$ into $y = \beta x + \epsilon$, we obtain:

\[
    y_i = \beta(\tilde{x}_i - u_i) + \epsilon_i = \beta \tilde{x} + (\epsilon - \beta u)
\]

As the measurement error in $x$ becomes part of the residual error term in the model, the exogeneity assumption of the Gauss-Markov theorem is violated as $cov(u, \tilde{x}) \neq 0$.
Thus, the ordinary least-squares (OLS) estimator of $\beta$ may not be the \textit{best linear unbiased estimator}.
Unlike the case of with measurement error, the OLS and MME estimators are different.
In fact, We will see that the OLS estimators of $\beta$ and $\sigma_\epsilon^2$ are biased. 
n order to obtain unbiased and consistent estimates, we would have to resort to the method of moments (MME) estimators instead. 

\subsection{OLS and MME for $\beta$}

Suppose we use the ordinary least-squares (OLS) estimator for $\beta$:

\[
\hat{\beta} = \frac{cov(\tilde{x, y})}{var(\tilde{x})} = \frac{cov(x+u, \beta x + \epsilon)}{var(x + u)}
\]

Because $\epsilon$, $u$ and $x$ are independent to each other, we can obtain

\[
\textrm{plim } \hat{\beta} = \frac{\beta \sigma^2_x}{\sigma^2_x + \sigma^2_u} = \lambda \beta
\]

where

\[
\lambda \equiv \frac{\sigma_x^2}{\sigma_x^2 + \sigma_u^2}
\]

This $\lambda$ is also called the reliability or signal-to-variance ratio.

Therefore, we can see that the OLS estimator $\hat{\beta}$ is biased towards zero because $0 < \lambda < 1$. The sign of the bias depends on the sign of the true $\beta$.

In order to obtain consistent estimates for $\beta$, we can use the MME estimator instead.
Suppose we have some prior knowledge on the measurement errors and have obtained the value of $\sigma_x^2$, $\sigma_u^2$ or $\lambda$.
Then we can apply the appropriate adjustment for the bias in the OLS $\hat{\beta}$ as $\sigma^2_{\tilde{x}} = \sigma^2_x + \sigma^2_u$, and $\sigma^2_{\tilde{x}}$ can be directly measured from the observed data. 

Some MME estimators for $\beta$ using the first and second moments alone are shown below:

\begin{table}[ht]
    \centering
    \caption{Various MME estimators for $\beta$}
    \begin{tabular}[t]{lcc}
        \hline
        Assumption&Method of Moments Estimator\\
        \hline
        $\sigma_x^2$ known&--\\
        $\sigma_u^2$ known&$\frac{\sigma^2_{\tilde{x}}}{\sigma^2_{\tilde{x}} - \sigma^2_u}$\\
        Reliability ratio $\lambda$ known&--\\
        \hline
    \end{tabular}
\end{table}%

For example, in practice, we can obtain the values of $\sigma_u^2$ via repeated measurements \cite{mmereport}.

\subsection{OLS and MME for $\sigma_\epsilon^2$}

The usual way in OLS to estimate $\sigma_\epsilon^2$ is to calculate $\hat{\sigma^2_\epsilon}$ as the sum of squares of the residuals divided by the degrees of freedom $n-d$. To find out what happens to $\hat{\sigma^2_\epsilon}$, we can first look at what happens to the estimated residual variance first \cite{lecturenotes}:

\begin{equation}
    \begin{split}
        \hat{\epsilon}  &= y - \hat{\beta} \tilde{x} \\
                        &= y - \hat{\beta}(x+u) \\
                        &= \epsilon - (y - \beta x) + y - \hat{\beta}x - \hat{\beta}u \\
                        &= \epsilon + (\beta - \hat{\beta})x - \hat{\beta}u
    \end{split} 
\end{equation}

Thus, we can see that the true error term is obfuscated by two additional sources of error. 

We can further obtain the OLS estimated variance as we assumed earlier that $\epsilon$, $x$ and $u$ are uncorrelated:

\[
    \textrm{plim } \hat{\sigma^2_\epsilon} = \sigma_\epsilon^2 + (1-\lambda)^2 \beta^2 \sigma_x^2 + \lambda^2 \beta^2 \sigma_u^2   
\]

In order to obtain the MME estimator for $\sigma^2_\epsilon$, similar to the case for $\hat{\beta}_{MME}$, if we have some prior knowledge on the observation error, we can obtain consistent estimates for $\sigma_x^2$, $\sigma_u$, $\lambda$ and $\beta$.
Thus, rearranging the expression for $\hat{\sigma^2_\epsilon}$ gives:

\[
    \hat{\sigma_\epsilon^2}_{MME} = \hat{\sigma^2_\epsilon}_{OLS} -  (1-\lambda)^2 \hat{\beta}_{MME}^2 \sigma_x^2 - \lambda^2 \hat{\beta}_{MME}^2 \sigma_u^2   
\]

\section{Methodology}

\subsection{Procedures}

The computer simulation method was employed in order to investigate the impacts of non-normal errors on the OLS and MME estimates.

For all simulations, we use a common $\beta_{truth}$, $n$, $x_{lo}$, $x{hi}$. $\beta_{truth}$ is the underlying ground truth $\beta$ of the model $y=\beta x + \epsilon$. $n$ is the number of observations used in each simulation trial. $x_{lo}$ and $x_{hi}$ are the lower bound and upper bound of the $x$ being sampled respectively.

In order to produce comparable results when using different error distributions, for each experiment, we first fix a certain $\sigma^2_u$ and $\sigma^2_\epsilon$.
Afterwards, for the observation error $u$, we choose a distribution where the observation errors $u_i$ are sampled from.
The model parameters of this distribution is chosen such that it has variance equal to $\sigma_u^2$.
Similarly, for the residual error $\epsilon$, we choose a distribution where the residual errors $\epsilon_i$ are sampled from.
The model parameters of this distribution is chosen such that it has variance equal to $\sigma_\epsilon^2$.

After determining which distributions to use, we perform the observation data generation step. First, we draw independently $n$ instances of explanatory variable $x_i$ from the uniform distribution defined by the bounds $x_{lo}$ and $x_{hi}$, $U(x_{lo}, x_{hi})$.
Afterwards, for each of the instances $x_i$, we compute $y_i = \beta_{truth} \cdot x_i + \epsilon_i$, where $\epsilon_i$ is drawn independently from the distribution for residual errors determined for this simulation trial.
For each $x_i$, we also compute the observed explanatory variable $\tilde{x_i} = x_i + u_i$, where observation error $u_i$ is added for each $x_i$. $u_i$ is drawn independently from the distribution for observation errors determined for this simulation trial.

After generating the observation data, we perform estimations based on the observation data. We perform both the OLS and MME estimations for $\beta$ and $\sigma_\epsilon^2$. For the MME estimations, we use the case where prior information on the observation error $\sigma_u^2$ is known. The relevant expressions for the estimators are listed below:

\begin{equation}
    \begin{split}
        \hat{\beta}_{OLS} &= \frac{cov(\tilde{x, y})}{var(\tilde{x})} = \frac{cov(x+u, \beta x + \epsilon)}{var(x + u)}\\
        \hat{\beta}_{MME} &= \cdots\\
        \hat{\sigma^2_\epsilon}_{OLS} &= \cdots\\
        \hat{\sigma^2_\epsilon}_{MME} &= \cdots\\
    \end{split}
\end{equation}

After computing the estimators, we compute the bias and squared error of each of these estimators when compared to the ground truth $\beta_{truth}$ and $\sigma^2_\epsilon$. 

For the same observation error distribution and residual error distribution, we perform the whole observation data generation and estimation procedures for multiple iterations.
This is to obtain more reliable conclusions on the mean bias (MBE) and mean squared error (MSE) of the estimators.

\begin{algorithm}[H]
    \SetAlgoLined
    Initialize $\beta_{truth}$, $n$, $x_{lo}$, $x_{hi}$\;
    \ForAll{$\sigma^2_u \in \{\textrm{list of observation variance}\}$}{
        \ForAll{$\sigma^2_\epsilon \in \{\textrm{list of residual variance}\}$}{
            Initialize $MSE_{\hat{\beta}_{OLS}}=0$, $MSE_{\hat{\beta}_{MME}}=0$, $MBE_{\hat{\beta}_{OLS}}=0$, $MBE_{\hat{\beta}_{MME}}=0$.\;
            Initialize $MSE_{{\hat{\sigma^2_\epsilon}}_{OLS}}=0$, $MSE_{{\hat{\sigma^2_\epsilon}}_{MME}}=0$, $MBE_{\hat{\sigma^2_\epsilon}_{OLS}}=0$, $MBE_{\hat{\sigma^2_\epsilon}_{MME}}=0$\;

            \For{10000 iterations}{
                Draw independently $n$ instances of explanatory variable $x_i$ from $U(x_{lo}, x_{hi})$\;
                Compute $n$ observations of $y_i = \beta_{truth} \cdot x_i + \epsilon_i$, where $\epsilon_i$ is drawn independently from a distribution with variance $\sigma^2_\epsilon$\;
                Compute $n$ instances of observed explanatory variable $\tilde{x}_i = x_i + u_i$, where observation $u_i$ is drawn independently from a distribution with variance $\sigma^2_u$\;
                Calculate $\hat{\beta}_{OLS}$, $\hat{\beta}_{MME}$, $\hat{\sigma^2_\epsilon}_{OLS}$, $\hat{\sigma^2_\epsilon}_{MSE}$ from the observations $\tilde{x}$, $y$, and prior information on observation error $\sigma^2_u$\;
                Update $MSE_{\hat{\beta}_{OLS}}$, $MSE_{\hat{\beta}_{MME}}$, $MBE_{\hat{\beta}_{OLS}}$, $MBE_{\hat{\beta}_{MME}}$, $MSE_{{\hat{\sigma^2_\epsilon}}_{OLS}}$, $MSE_{{\hat{\sigma^2_\epsilon}}_{MME}}$, $MBE_{\hat{\sigma^2_\epsilon}_{OLS}}$, $MBE_{\hat{\sigma^2_\epsilon}_{MME}}$\;
            }
        }
    }
    \caption{Computer simulation procedure}
    \label{alg:Algo}
\end{algorithm}

The computer simulation procedures are summarized in procedure \ref{alg:Algo}. 

\subsection{Choice of distribution}

In order to investigate the impacts on non-normality on the estimators, there is a need to use non-normal distributions to sample the observation error $u$ and residual error $\epsilon$.
In order to satisfy the assumptions of the errors-in-variables model, the distributions used to generate the error terms should have mean zero.
In our experiment procedures, we also have to derive the model parameters from a fixed $\sigma_u^2$ and $\sigma_\epsilon^2$.
Thus, in order to ease calculation, the distribution chosen should have finite variance and the variance is in a closed form such that the derivation of model parameters from a fixed variance is easy to compute.

Taking the above factors into consideration, we decided to use the following distributions: The normal/Gaussian distribution, the Student's $t$ distribution, and the $\chi^2$ distribution re-centered at mean $0$.
They are chosen because they demonstrate features not seen in the normal distribution - the Student's $t$ distribution is known to have heavier tails than the normal distribution, and the $\chi^2$ distribution is known to be skewed.
They pose substantial differences from the normal distribution which has light tails and is symmetric at $0$.

We will then perform the simulations using these distributions as the underlying sampling distributions of the observation error $u$ and residual error $\epsilon$, as described in the previous subsection.

\subsection{Experiment parameters used}

To produce the results in the subsequent section, we used the following parameters: $n=30$, $\beta_{truth}=2$, $x_{lo} = -20$, $x_{hi} = 20$.
For the simulation procedures, we used $\sigma^2_u \in {0, 1.5, 2, 2.5, 3.0, 4.5, 6.0, 8.0, 10.0}$, and $\sigma_\epsilon^2 \in {1.5, 2, 2.5}$.
We are unable to experiment with variances from $(0, 1]$ because the variance of the Student's $t$ distribution is only defined when $\sigma^2 > 1$.

For each combination of observation error distribution and residual error distribution, we performed $10,000$ iterations to produce the MBE and MSE results.

\subsection{Implementation details}

The codes were implemented in Python in a Jupyter notebook. We used the sampling distribution implementations provided in the \verb|numpy.random| package. The detailed implementation and source code can be found in the appendix.

\section{Results and discussions}

We split this section into several subsections.

First, we describe the results for the estimation of $\beta$.
We consider both the MBE and MSE for the OLS and MME estimators of $\beta$. 
We compare the simulation outcomes for different distributions of residual error for when there is absence of observation error (i.e. $\sigma^2_u = 0$).
For the cases under the presence of observation error (i.e. $\sigma^2_u > 0$), we compare the MBE and MSE for different distributions (Normal, Student's $t$, $\chi^2$) while maintaining the same variance.

Afterwards, we similarly describe the results for the estimation of $\sigma^2_\epsilon$.
We consider both the MBE and MSE for the OLS and MME estimators of $\sigma^2$.
We similarly compare the simulation outcomes under the cases of absence of observation error, and cases under the presence of observation error of different distributions respectively.

\subsection{Estimation of $\beta$}

We use the following estimators of $\beta$ and apply them to the generated observation data $(\tilde{x}_i, y_i)$:

\begin{equation}
    \begin{split}
        \hat{\beta}_{OLS} &= \frac{cov(\tilde{x, y})}{var(\tilde{x})} = \frac{cov(x+u, \beta x + \epsilon)}{var(x + u)}\\
        \hat{\beta}_{MME} &= \cdots\\
    \end{split}
\end{equation}

\subsubsection{Absence of observation error}

Under the absence of observation error, the OLS and MME estimators of $\beta$ are the same.

\begin{table}[ht]
    \centering
    \caption{Estimation MBE of $\beta$ when $\sigma^2_u=0$, under different distributions and variance of $\epsilon$.}
    \label{Tab:MBE_absence}
    \begin{tabular}[t]{lccc}
        \hline
        &Normal&Student's $t$&$\chi^2$\\
        \hline
        $\sigma^2_\epsilon = 1.5$&-0.000260& 0.000252& 0.000052\\
        $\sigma^2_\epsilon = 2.0$& 0.000151& 0.000114&-0.000356\\
        $\sigma^2_\epsilon = 2.5$&-0.000176&-0.000105& 0.000112\\
        \hline
    \end{tabular}
\end{table}

Table \ref{Tab:MBE_absence} summarizes the MBE of $\beta$ when $\sigma^2_u=0$ for different error distributions.
There is no clear trend of increasing/decreasing pattern of the MBE for each of the distributions. 
This is mostly because the bias is centered at zero, leading to the signs of bias to be different between simulation trials, which cancel out each other in the calculation of MBE. 

\begin{table}[ht]
    \centering
    \caption{Estimation MSE of $\beta$ when $\sigma^2_u=0$, under different distributions and variance of $\epsilon$.}
    \label{Tab:MSE_absence}
    \begin{tabular}[t]{lccc}
        \hline
        &Normal&Student's $t$&$\chi^2$\\
        \hline
        $\sigma^2_\epsilon = 1.5$&0.000352&0.000351&0.000343\\
        $\sigma^2_\epsilon = 2.0$&0.000470&0.000478&0.000458\\
        $\sigma^2_\epsilon = 2.5$&0.000588&0.000594&0.000573\\
        \hline
    \end{tabular}
\end{table}

Table \ref{Tab:MSE_absence} summarizes the MSE of $\beta$ when $\sigma^2_u=0$ for different error distributions.
There is a clear trend of increasing of the MSE for each of the distributions as $\sigma^2_\epsilon$ increases. 
Furthermore, we can observe that the MSE for $\chi^2$ distribution is the least in all 3 experiments, and the MSE for Student's $t$ distribution is the greatest in 2 out of 3 experiments.

\subsubsection{Normal-distributed observation error and normal-distributed residual error}

Under the presence of observation error, the OLS and MME estimators of $\beta$ are different.
We would expect the MME estimator to have lower absolute bias and lower MSE because under the presence of observation error, the OLS estimator is inconsistent but the MME estimator is consistent.

Table \ref{Tab:MBE_normal} and Table \ref{Tab:MSE_normal} summarize the simulated MBE and MSE of the estimation of $\beta$ respectively, under normal-distributed errors.

\begin{table}[ht]
    \centering
    \caption{Estimation MBE of $\hat{\beta}_{OLS}$ and $\hat{\beta}_{MME}$ under normal-distributed observation error with different variances $\sigma^2_u$, and under normal-distributed residual error with different variances $\sigma^2_\epsilon$.}
    \label{Tab:MBE_normal}
    \begin{tabular}[t]{lcccccc}
        \hline
        &\multicolumn{2}{c}{$\sigma^2_\epsilon=1.5$}&\multicolumn{2}{c}{$\sigma^2_\epsilon=2.0$}&\multicolumn{2}{c}{$\sigma^2_\epsilon=2.5$}\\
        \hline
        &$\hat{\beta}_{OLS}$&$\hat{\beta}_{MME}$&$\hat{\beta}_{OLS}$&$\hat{\beta}_{MME}$&$\hat{\beta}_{OLS}$&$\hat{\beta}_{MME}$\\
        \hline
        $\sigma^2_u = 1.5$&-0.019467&0.000711&-0.018743&0.001453&-0.018929&0.001258\\
        $\sigma^2_u = 2.0$&-0.025206&0.001655&-0.023977&0.002920&-0.025265&0.001602\\
        $\sigma^2_u = 2.5$&-0.030908&0.002607&-0.030422&0.003106&-0.030886&0.002635\\
        $\sigma^2_u = 3.0$&-0.036965&0.003140&-0.036627&0.003527&-0.036902&0.003217\\
        $\sigma^2_u = 4.5$&-0.055003&0.004770&-0.056174&0.003501&-0.056439&0.003171\\
        $\sigma^2_u = 6.0$&-0.072019&0.007202&-0.071406&0.007854&-0.073418&0.005616\\
        $\sigma^2_u = 8.0$&-0.096652&0.007750&-0.096022&0.008548&-0.096813&0.007639\\
        $\sigma^2_u = 10.0$&-0.119142&0.010226&-0.121257&0.007779&-0.117934&0.011878\\
    \end{tabular}
\end{table}

\begin{table}[ht]
    \centering
    \caption{Estimation MSE of $\hat{\beta}_{OLS}$ and $\hat{\beta}_{MME}$ under normal-distributed observation error with different variances $\sigma^2_u$, and under normal-distributed residual error with different variances $\sigma^2_\epsilon$.}
    \label{Tab:MSE_normal}
    \begin{tabular}[t]{lcccccc}
        \hline
        &\multicolumn{2}{c}{$\sigma^2_\epsilon=1.5$}&\multicolumn{2}{c}{$\sigma^2_\epsilon=2.0$}&\multicolumn{2}{c}{$\sigma^2_\epsilon=2.5$}\\
        \hline
        &$\hat{\beta}_{OLS}$&$\hat{\beta}_{MME}$&$\hat{\beta}_{OLS}$&$\hat{\beta}_{MME}$&$\hat{\beta}_{OLS}$&$\hat{\beta}_{MME}$\\
        \hline
        $\sigma^2_u = 1.5$&0.002049&0.001761&0.002120&0.001862&0.002254&0.001992\\
        $\sigma^2_u = 2.0$&0.002801&0.002331&0.002747&0.002338&0.003020&0.002552\\
        $\sigma^2_u = 2.5$&0.003447&0.002739&0.003563&0.002895&0.003810&0.003124\\
        $\sigma^2_u = 3.0$&0.004320&0.003313&0.004423&0.003452&0.004584&0.003595\\
        $\sigma^2_u = 4.5$&0.007218&0.004996&0.007379&0.005007&0.007404&0.004989\\
        $\sigma^2_u = 6.0$&0.010421&0.006639&0.010245&0.006529&0.010711&0.006687\\
        $\sigma^2_u = 8.0$&0.015781&0.008820&0.015708&0.008875&0.016050&0.009091\\
        $\sigma^2_u = 10.0$&0.022013&0.011578&0.022421&0.011359&0.021891&0.011826\\
    \end{tabular}
\end{table}

We first focus on the observations of the MBE and MSE of $\hat{\beta}_{OLS}$.

Firstly, we observe that the MBE of $\hat{\beta}_{OLS}$ is negative.
This is result is expected because $\textrm{plim } \hat{\beta}_{OLS} = \frac{\beta \sigma^2_x}{\sigma^2_x + \sigma^2_u} = \lambda \cdot \beta_{truth}$ and is biased towards zero.
In this simulation, $\beta_{truth} = 2 > 0$.
Thus, the bias of $\hat{\beta}_{OLS}$ is expected to be negative, which is consistent with the simulation findings.
Second, we observe that the MBE of $\hat{\beta}_{OLS}$ decreases as $\sigma^2_u$ increases.
This is also consistent with the literature as $\lambda \equiv \frac{\sigma_x^2}{\sigma_x^2 + \sigma_u^2}$.
When $\sigma^2_u$ increases, $\lambda$ decreases, and the bias $(\lambda-1) \cdot \beta_{truth}$ decreases.
Third, we observe that the MSE of $\hat{\beta}_{OLS}$ increases as $\sigma^2_u$ increases. Intuitively, this means that the error of the estimation of $\beta$ increases as the observation data is dirtier. 

Now, we turn our attention to the observations of the MBE and MSE of $\hat{\beta}_{MME}$, and compare them with $\hat{\beta}_{OLS}$.

Firstly, we observe that the absolute value of MBE and MSE of $\hat{\beta}_{MME}$ are both lower than that of $\hat{\beta}_{OLS}$ at the same $\sigma^2_u$ and $\sigma^2_\epsilon$.
This is expected because $\hat{\beta}_{MME}$ is a consistent estimator of $\beta$ while $\hat{\beta}_{OLS}$ is not.
Second, we observe that the MBE of $\hat{\beta}_{MME}$ (roughly) increases as $\sigma^2_u$ increases.
Third, we observe that the MSE of $\hat{\beta}_{MME}$ increases as $\sigma^2_u$ increases. This is similar to the case of $\hat{\beta}_{OLS}$.

\subsubsection{Student's $t$-distributed observation error and Student's $t$-distributed residual error}

We perform the similar simulations for Student's $t$ distributed observation error and residual error for different variances.
Elementary statistical theory tells us that the variance for a Student's $t$ distribution with $\nu > 0$ degrees of freedom has variance equal to $\frac{\nu}{\nu-2}$.
Taking inverse of the equation $\sigma^2 = \frac{\nu}{\nu-2}$ gives $\nu = 2 \cdot \frac{\sigma^2}{\sigma^2-1}$.
Note that the range of $\sigma^2_u$ and $\sigma^2_\epsilon$ are specially chosen such that $\nu > 2$ because the Student's $t$ distribution only has finite variance when $\nu > 2$.
Thus, in the tables shown below, when we write $\sigma^2 = \delta$, it means the simulation trial was performed using the Student's $t$ distribution with $2 \cdot \frac{\delta}{\delta-1}$ degrees of freedom.

Table \ref{Tab:MBE_t} and Table \ref{Tab:MSE_t} summarize the simulated MBE and MSE of the estimation of $\beta$ respectively, under Student's $t$-distributed errors.

\begin{table}[ht]
    \centering
    \caption{Estimation MBE of $\hat{\beta}_{OLS}$ and $\hat{\beta}_{MME}$ under Student's $t$-distributed observation error with different variances $\sigma^2_u$, and under Student's $t$-distributed residual error with different variances $\sigma^2_\epsilon$.}
    \label{Tab:MBE_t}
    \begin{tabular}[t]{lcccccc}
        \hline
        &\multicolumn{2}{c}{$\sigma^2_\epsilon=1.5$}&\multicolumn{2}{c}{$\sigma^2_\epsilon=2.0$}&\multicolumn{2}{c}{$\sigma^2_\epsilon=2.5$}\\
        \hline
        &$\hat{\beta}_{OLS}$&$\hat{\beta}_{MME}$&$\hat{\beta}_{OLS}$&$\hat{\beta}_{MME}$&$\hat{\beta}_{OLS}$&$\hat{\beta}_{MME}$\\
        \hline
        $\sigma^2_u = 1.5$&-0.018770&0.001429&-0.019233&0.000955&-0.018866&0.001329\\
        $\sigma^2_u = 2.0$&-0.024866&0.002008&-0.025077&0.001785&-0.025835&0.001001\\
        $\sigma^2_u = 2.5$&-0.030725&0.002796&-0.030205&0.003343&-0.030965&0.002552\\
        $\sigma^2_u = 3.0$&-0.035638&0.004529&-0.035464&0.004732&-0.034195&0.006041\\
        $\sigma^2_u = 4.5$&-0.047532&0.012747&-0.047863&0.012389&-0.047730&0.012474\\
        $\sigma^2_u = 6.0$&-0.053997&0.026746&-0.054018&0.026632&-0.055230&0.025393\\
        $\sigma^2_u = 8.0$&-0.063497&0.044623&-0.062978&0.045236&-0.060738&0.047766\\
        $\sigma^2_u = 10.0$&-0.066641&0.070167&-0.065806&0.070902&-0.065357&0.071511\\
    \end{tabular}
\end{table}

\begin{table}[ht]
    \centering
    \caption{Estimation MSE of $\hat{\beta}_{OLS}$ and $\hat{\beta}_{MME}$ under Student's $t$-distributed observation error with different variances $\sigma^2_u$, and under Student's $t$-distributed residual error with different variances $\sigma^2_\epsilon$.}
    \label{Tab:MSE_t}
    \begin{tabular}[t]{lcccccc}
        \hline
        &\multicolumn{2}{c}{$\sigma^2_\epsilon=1.5$}&\multicolumn{2}{c}{$\sigma^2_\epsilon=2.0$}&\multicolumn{2}{c}{$\sigma^2_\epsilon=2.5$}\\
        \hline
        &$\hat{\beta}_{OLS}$&$\hat{\beta}_{MME}$&$\hat{\beta}_{OLS}$&$\hat{\beta}_{MME}$&$\hat{\beta}_{OLS}$&$\hat{\beta}_{MME}$\\
        \hline
        $\sigma^2_u = 1.5$&0.002021&0.001761&0.002213&0.001939&0.002321&0.002065\\
        $\sigma^2_u = 2.0$&0.002921&0.002473&0.003258&0.002809&0.003236&0.002744\\
        $\sigma^2_u = 2.5$&0.004083&0.003415&0.004288&0.003668&0.004529&0.003870\\
        $\sigma^2_u = 3.0$&0.005745&0.004929&0.005835&0.005041&0.005557&0.004851\\
        $\sigma^2_u = 4.5$&0.011736&0.010821&0.011447&0.010442&0.010748&0.009719\\
        $\sigma^2_u = 6.0$&0.013428&0.013042&0.013088&0.012611&0.014464&0.013965\\
        $\sigma^2_u = 8.0$&0.018133&0.019265&0.017601&0.018818&0.017577&0.019249\\
        $\sigma^2_u = 10.0$&0.020794&0.025946&0.019452&0.024433&0.019140&0.024228\\
    \end{tabular}
\end{table}

We first focus on the observations based solely on the Student's $t$-distributed error simulations. This will be followed by comparing the results in normal-distributed errors and Student's $t$-distributed errors. 

Firstly, similar to the normal-distributed errors case, we observe that the MBE of $\hat{\beta}_{OLS}$ is negative and decreasing in $\sigma^2_u$,
and that the MBE of $\hat{\beta}_{MME}$ is positive and increasing in $\sigma^2_u$.
The MSE of $\hat{\beta}_{OLS}$ and $\hat{\beta}_{MME}$ are both increasing in $\sigma^2_u$.

However, when comparing $\hat{\beta}_{OLS}$ and $\hat{\beta}_{MME}$,
we find that switching from $\hat{\beta}_{OLS}$ to $\hat{\beta}_{MME}$ does not provide that much of a decrease in absolute MBE or MSE
compared to the normal-distributed errors case.
This situation is much more apparent in large values of $\sigma^2_u$.
For MBE, when $\sigma^2_u = 10.0$, we observe that the absolute value of the MBE of $\hat{\beta}_{MME}$ is in fact greater than that of $\hat{\beta}_{OLS}$.
Similarly for MSE, when $\sigma^2_u \in \{8.0, 10.0\}$, we observe that the MSE of $\hat{\beta}_{MME}$ is greater than that of $\hat{\beta}_{OLS}$.

%%%%%%%%%%%%%%%%

To compare the findings between the normal-distributed errors case and Student's $t$-distributed errors case,
it is useful to take the cell difference between the estimation MBE in Tables \ref{Tab:MBE_normal} and \ref{Tab:MBE_t},
and estimation MSE in Tables \ref{Tab:MSE_normal} and \ref{Tab:MSE_t}.
We define the difference here as $(\textrm{value from Student's } t) - (\textrm{value from normal})$. 
These differences are tabulated in Tables \ref{Tab:MBE_diff_t_normal} and \ref{Tab:MSE_diff_t_normal} respectively.

\begin{table}[ht]
    \centering
    \caption{Difference between Student's $t$ error case and normal-distributed error case: Estimation MBE of $\hat{\beta}_{OLS}$ and $\hat{\beta}_{MME}$ under different $\sigma^2_u$ and $\sigma^2_\epsilon$.}
    \label{Tab:MBE_diff_t_normal}
    \begin{tabular}[t]{lcccccc}
        \hline
        &\multicolumn{2}{c}{$\sigma^2_\epsilon=1.5$}&\multicolumn{2}{c}{$\sigma^2_\epsilon=2.0$}&\multicolumn{2}{c}{$\sigma^2_\epsilon=2.5$}\\
        \hline
        &$\hat{\beta}_{OLS}$&$\hat{\beta}_{MME}$&$\hat{\beta}_{OLS}$&$\hat{\beta}_{MME}$&$\hat{\beta}_{OLS}$&$\hat{\beta}_{MME}$\\
        \hline
        $\sigma^2_u = 1.5$&-0.000028&-0.000000&0.000093&0.000077&0.000067&0.000073\\
        $\sigma^2_u = 2.0$&0.000120&0.000142&0.000511&0.000471&0.000215&0.000191\\
        $\sigma^2_u = 2.5$&0.000636&0.000676&0.000725&0.000773&0.000719&0.000746\\
        $\sigma^2_u = 3.0$&0.001425&0.001616&0.001412&0.001589&0.000973&0.001257\\
        $\sigma^2_u = 4.5$&0.004518&0.005825&0.004068&0.005434&0.003343&0.004731\\
        $\sigma^2_u = 6.0$&0.003007&0.006403&0.002843&0.006083&0.003753&0.007279\\
        $\sigma^2_u = 8.0$&0.002352&0.010444&0.001893&0.009943&0.001527&0.010159\\
        $\sigma^2_u = 10.0$&-0.001219&0.014368&-0.002969&0.013074&-0.002751&0.012402\\
    \end{tabular}
\end{table}

\begin{table}[ht]
    \centering
    \caption{Difference between Student's $t$ error case and normal-distributed error case: Estimation MSE of $\hat{\beta}_{OLS}$ and $\hat{\beta}_{MME}$ under different $\sigma^2_u$ and $\sigma^2_\epsilon$.}
    \label{Tab:MSE_diff_t_normal}
    \begin{tabular}[t]{lcccccc}
        \hline
        &\multicolumn{2}{c}{$\sigma^2_\epsilon=1.5$}&\multicolumn{2}{c}{$\sigma^2_\epsilon=2.0$}&\multicolumn{2}{c}{$\sigma^2_\epsilon=2.5$}\\
        \hline
        &$\hat{\beta}_{OLS}$&$\hat{\beta}_{MME}$&$\hat{\beta}_{OLS}$&$\hat{\beta}_{MME}$&$\hat{\beta}_{OLS}$&$\hat{\beta}_{MME}$\\
        \hline
        $\sigma^2_u = 1.5$&0.000697&0.000718&-0.000491&-0.000498&0.000063&0.000072\\
        $\sigma^2_u = 2.0$&0.000340&0.000354&-0.001099&-0.001135&-0.000570&-0.000601\\
        $\sigma^2_u = 2.5$&0.000183&0.000189&0.000218&0.000237&-0.000079&-0.000083\\
        $\sigma^2_u = 3.0$&0.001327&0.001390&0.001162&0.001205&0.002707&0.002824\\
        $\sigma^2_u = 4.5$&0.007471&0.007977&0.008311&0.008888&0.008708&0.009303\\
        $\sigma^2_u = 6.0$&0.018022&0.019544&0.017388&0.018778&0.018188&0.019777\\
        $\sigma^2_u = 8.0$&0.033155&0.036873&0.033045&0.036688&0.036074&0.040126\\
        $\sigma^2_u = 10.0$&0.052501&0.059941&0.055451&0.063122&0.052577&0.059633\\
    \end{tabular}
\end{table}

It should be noted that when the variance of a Student's $t$ distribution is low, the number of degrees of freedom of the distribution is high. 
From elementary statiscal theory we know that as the number of degrees of freedom of a Student's $t$ distribution increases, it converges to the normal distribution.
Thus, for some low fixed variance, we should expect similar findings from a Student's $t$ distribution and normal distribution with that variance.

We first focus on the comparisons in MBE.
For $\hat{\beta}_{OLS}$, we observe that the difference is first close to zero, then increases to a certain point (around at $\sigma^2_u = 4.5$), then again starts to decrease.
As the bias of $\hat{\beta}_{OLS}$ is negative in both Student's $t$-distributed errors and normal-distributed errors, this shows that $\hat{\beta}_{OLS}$ under Student's $t$ errors is biased downwards less.
On the other hand, for $\hat{\beta}_{MME}$, we observe that the difference is increasing throughout as $\sigma^2_u$ is increased. 

We then focus on the comparisons in MSE.
Although the trends for small $\sigma^2_u$ is not clear, for both $\hat{\beta}_{OLS}$ and $\hat{\beta}_{MME}$
we observe that the MSE under Student's $t$-distributed errors is in general greater than that under normal-distributed errors,
and that this difference is in general increasing in $\sigma^2_u$.

\subsubsection{$\chi^2$-distributed observation error}

We perform the similar simulations for $\chi^2$ distributed observation error and residual error for different variances.
From elementary statistical theory, a $\chi^2$ distribution with $k > 0$ degrees of freedom has mean $k$ and variance $2k$.
To let the distribution follow the model assumptions to have mean zero, we subtract $k$ from each value drawn from the distribution.
Thus, when we write $\sigma^2 = \delta$, it means the simulation trial was performed using the $\chi^2$ distribution with $2 \cdot \frac{\delta}{2}$ degrees of freedom minus $\frac{\delta}{2}$ to recenter the mean to zero.

Table \ref{Tab:MBE_chi} and Table \ref{Tab:MSE_chi} summarize the simulated MBE and MSE of the estimation of $\beta$ respectively, under $\chi^2$-distributed errors.

\begin{table}[ht]
    \centering
    \caption{Estimation MBE of $\hat{\beta}_{OLS}$ and $\hat{\beta}_{MME}$ under $\chi^2$-distributed observation error with different variances $\sigma^2_u$ and residual error with different variances $\sigma^2_\epsilon$.}
    \label{Tab:MBE_chi}
    \begin{tabular}[t]{lcccccc}
        \hline
        &\multicolumn{2}{c}{$\sigma^2_\epsilon=1.5$}&\multicolumn{2}{c}{$\sigma^2_\epsilon=2.0$}&\multicolumn{2}{c}{$\sigma^2_\epsilon=2.5$}\\
        \hline
        &$\hat{\beta}_{OLS}$&$\hat{\beta}_{MME}$&$\hat{\beta}_{OLS}$&$\hat{\beta}_{MME}$&$\hat{\beta}_{OLS}$&$\hat{\beta}_{MME}$\\
        \hline
        $\sigma^2_u = 1.5$&-0.018714&0.001479&-0.018709&0.001491&-0.018640&0.001558\\
        $\sigma^2_u = 2.0$&-0.025420&0.001430&-0.024128&0.002761&-0.025012&0.001851\\
        $\sigma^2_u = 2.5$&-0.030573&0.002949&-0.031135&0.002355&-0.031329&0.002149\\
        $\sigma^2_u = 3.0$&-0.036298&0.003864&-0.036901&0.003226&-0.037024&0.003085\\
        $\sigma^2_u = 4.5$&-0.054917&0.004847&-0.055502&0.004227&-0.054086&0.005756\\
        $\sigma^2_u = 6.0$&-0.072913&0.006221&-0.072631&0.006482&-0.072691&0.006443\\
        $\sigma^2_u = 8.0$&-0.097304&0.007052&-0.095563&0.009114&-0.097776&0.006451\\
        $\sigma^2_u = 10.0$&-0.119601&0.009722&-0.119559&0.009724&-0.117377&0.012370\\
    \end{tabular}
\end{table}

\begin{table}[ht]
    \centering
    \caption{Estimation MSE of $\hat{\beta}_{OLS}$ and $\hat{\beta}_{MME}$ under $\chi^2$-distributed observation error with different variances $\sigma^2_u$ and residual error with different variances $\sigma^2_\epsilon$.}
    \label{Tab:MSE_chi}
    \begin{tabular}[t]{lcccccc}
        \hline
        &\multicolumn{2}{c}{$\sigma^2_\epsilon=1.5$}&\multicolumn{2}{c}{$\sigma^2_\epsilon=2.0$}&\multicolumn{2}{c}{$\sigma^2_\epsilon=2.5$}\\
        \hline
        &$\hat{\beta}_{OLS}$&$\hat{\beta}_{MME}$&$\hat{\beta}_{OLS}$&$\hat{\beta}_{MME}$&$\hat{\beta}_{OLS}$&$\hat{\beta}_{MME}$\\
        \hline
        $\sigma^2_u = 1.5$&0.002214&0.001963&0.002375&0.002129&0.002378&0.002131\\
        $\sigma^2_u = 2.0$&0.003024&0.002552&0.002964&0.002557&0.003111&0.002658\\
        $\sigma^2_u = 2.5$&0.003698&0.003029&0.003881&0.003179&0.003957&0.003242\\
        $\sigma^2_u = 3.0$&0.004428&0.003481&0.004697&0.003716&0.004774&0.003782\\
        $\sigma^2_u = 4.5$&0.007516&0.005330&0.007542&0.005260&0.007606&0.005527\\
        $\sigma^2_u = 6.0$&0.011204&0.007381&0.011033&0.007194&0.011208&0.007384\\
        $\sigma^2_u = 8.0$&0.016869&0.009983&0.016494&0.009954&0.016978&0.009951\\
        $\sigma^2_u = 10.0$&0.022575&0.012067&0.022936&0.012550&0.022595&0.012841\\
    \end{tabular}
\end{table}

For the MBE and MSE, we have similar observations to the normal-distributed errors and Student's $t$-distributed errors cases,
where the MBE of $\hat{\beta}_{OLS}$ is negative and decreasing in $\sigma^2_u$, 
and the MBE of $\hat{\beta}_{MME}$ is positive and increasing in $\sigma^2_u$.
Similar to before, the MSE of $\hat{\beta}_{OLS}$ and $\hat{\beta}_{MME}$ are both increasing in $\sigma^2_u$.

To compare findings between the normal-distributed errors and $\chi^2$-distributed errors case,
we again take the cell difference from the two distributions in their MBE and MSE respectively.
The difference here is $(\textrm{value from } \chi^2) - (\textrm{value from normal})$. 
These differences are tabulated in Tables \ref{Tab:MBE_diff_chi_normal} and \ref{Tab:MSE_diff_chi_normal} respectively.

\begin{table}[ht]
    \centering
    \caption{Difference between $\chi^2$-distributed error case and normal-distributed error case: Estimation MBE of $\hat{\beta}_{OLS}$ and $\hat{\beta}_{MME}$ under different $\sigma^2_u$ and $\sigma^2_\epsilon$.}
    \label{Tab:MBE_diff_chi_normal}
    \begin{tabular}[t]{lcccccc}
        \hline
        &\multicolumn{2}{c}{$\sigma^2_\epsilon=1.5$}&\multicolumn{2}{c}{$\sigma^2_\epsilon=2.0$}&\multicolumn{2}{c}{$\sigma^2_\epsilon=2.5$}\\
        \hline
        &$\hat{\beta}_{OLS}$&$\hat{\beta}_{MME}$&$\hat{\beta}_{OLS}$&$\hat{\beta}_{MME}$&$\hat{\beta}_{OLS}$&$\hat{\beta}_{MME}$\\
        \hline
        $\sigma^2_u = 1.5$&0.000752&0.000768&0.000034&0.000038&0.000289&0.000301\\
        $\sigma^2_u = 2.0$&-0.000214&-0.000225&-0.000150&-0.000159&0.000253&0.000249\\
        $\sigma^2_u = 2.5$&0.000335&0.000342&-0.000712&-0.000751&-0.000442&-0.000486\\
        $\sigma^2_u = 3.0$&0.000667&0.000724&-0.000274&-0.000301&-0.000122&-0.000133\\
        $\sigma^2_u = 4.5$&0.000086&0.000077&0.000672&0.000725&0.002353&0.002585\\
        $\sigma^2_u = 6.0$&-0.000895&-0.000981&-0.001225&-0.001371&0.000728&0.000827\\
        $\sigma^2_u = 8.0$&-0.000651&-0.000698&0.000460&0.000566&-0.000963&-0.001188\\
        $\sigma^2_u = 10.0$&-0.000459&-0.000504&0.001698&0.001944&0.000557&0.000492\\
    \end{tabular}
\end{table}

\begin{table}[ht]
    \centering
    \caption{Difference between $\chi^2$-distributed error case and normal-distributed error case: Estimation MSE of $\hat{\beta}_{OLS}$ and $\hat{\beta}_{MME}$ under different $\sigma^2_u$ and $\sigma^2_\epsilon$.}
    \label{Tab:MSE_diff_chi_normal}
    \begin{tabular}[t]{lcccccc}
        \hline
        &\multicolumn{2}{c}{$\sigma^2_\epsilon=1.5$}&\multicolumn{2}{c}{$\sigma^2_\epsilon=2.0$}&\multicolumn{2}{c}{$\sigma^2_\epsilon=2.5$}\\
        \hline
        &$\hat{\beta}_{OLS}$&$\hat{\beta}_{MME}$&$\hat{\beta}_{OLS}$&$\hat{\beta}_{MME}$&$\hat{\beta}_{OLS}$&$\hat{\beta}_{MME}$\\
        \hline
        $\sigma^2_u = 1.5$&0.000165&0.000202&0.000255&0.000267&0.000124&0.000139\\
        $\sigma^2_u = 2.0$&0.000223&0.000221&0.000216&0.000219&0.000090&0.000105\\
        $\sigma^2_u = 2.5$&0.000251&0.000290&0.000318&0.000285&0.000147&0.000118\\
        $\sigma^2_u = 3.0$&0.000108&0.000167&0.000274&0.000264&0.000190&0.000188\\
        $\sigma^2_u = 4.5$&0.000298&0.000334&0.000164&0.000253&0.000202&0.000538\\
        $\sigma^2_u = 6.0$&0.000783&0.000742&0.000788&0.000666&0.000498&0.000697\\
        $\sigma^2_u = 8.0$&0.001088&0.001163&0.000785&0.001079&0.000928&0.000860\\
        $\sigma^2_u = 10.0$&0.000562&0.000489&0.000516&0.001191&0.000705&0.001015\\
    \end{tabular}
\end{table}

When comparing findings from $\chi^2$-distributed errors and normal-distributed errors, it should be noted that the $\chi^2$ distribution recentered at mean zero converges to a normal distribution with the same variance as the number of degrees of freedom is large.

For the MBE, there seems to be no clear comparison between the normal-distributed and $\chi^2$-distributed cases,
because the sign of the difference fluctuates in $\hat{\beta}_{OLS}$, $\hat{\beta}_{MME}$ and different $\sigma^2_u$.

For the MSE, the difference is always positive, meaning that the MSE for $\chi^2$-distributed case is always higher than normal-distributed case.
The difference seems to be increasing in $\sigma^2_u$, but the trend is not that clear. 

\subsection{Estimation of $\sigma^2_\epsilon$}

We use the following estimators of $\sigma^2_\epsilon$ and apply them to the generated observation data $(\tilde{x}_i, y_i)$:

\begin{equation}
    \begin{split}
        \hat{\sigma^2_\epsilon}_{OLS} &= \cdots\\
        \hat{\sigma^2_\epsilon}_{MME} &= \cdots\\
    \end{split}
\end{equation}

\subsubsection{Absence of observation error}

\subsubsection{Normal-distributed observation error}

\subsubsection{Student's $t$-distributed observation error}

\subsubsection{$\chi^2$-distributed observation error}

\section{Conclusion}

\subsection{Further work}

\section{Acknowledgments}

\section{Appendix}

\begin{thebibliography}{9}

\bibitem{lecturenotes}
    Steve Pischke.
    \textit{Lecture Notes on Measurement Error}.
    2007.

\bibitem{mmereport}
    Jonathan Gillard.
    \textit{Method of Moments Estimation in Linear Regression with Errors in both Variables}.
    2014.

\end{thebibliography}

\end{document}

